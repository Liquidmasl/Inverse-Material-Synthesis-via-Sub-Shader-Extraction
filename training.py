# -*- coding: utf-8 -*-
"""TrainingOnlyVer2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cmgqsNTS8qiqrKoq-65VROnD3kOleTv7
"""
import re

from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset
import torch
import torch.nn.functional as F  # Parameterless functions, like (some) activation functions
from torch import optim  # For optimizers like SGD, Adam, etc.
from torch import nn  # All neural network modules
from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.
from tqdm import tqdm  # For nice progress bar!
import wandb
import torch
import torchvision
import time
from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.
from tqdm import tqdm  # For nice progress bar!
import numpy as np
import pandas as pd
import os
import torch.nn.init as init

class BlenderSuperShaderRendersDataset(Dataset):
    def __init__(self, param_dir, gram_directory, transform=None, add_key_colors_to_input=False, prefetched_path=None, safe_or_load_prefetch=None):
        self.param_dir = param_dir
        self.gram_directory = gram_directory
        self.transform = transform
        self.add_key_colors_to_input = add_key_colors_to_input
        self.gram_path_cols = []
        self.dataset = None
        self.feature_mask = None

        self.num_params = 41

        if prefetched_path is None:
            self.init_dataframe()
        elif safe_or_load_prefetch == "safe":
            self.init_dataframe(save_df_to=prefetched_path)
        elif safe_or_load_prefetch == "load":
            self.dataset = pd.read_csv(prefetched_path, header=[0, 1])
        else:
            print("Invalid save_or_load_prefetch parameter. Use 'safe' or 'load'")
            raise ValueError("Invalid save_or_load_prefetch parameter. Use 'safe' or 'load'")


    def init_dataframe(self, save_df_to=None):
        samples = set([x.split('_')[0] for x in os.listdir(self.gram_directory)])
        self.gram_path_cols = [f"gra_{i}_path" for i in range(5)]

        mult_idx_tuples = []

        rows = []
        for sample in tqdm(samples, desc="Loading samples into dataset"):
            row = pd.read_csv(os.path.join(self.param_dir, f"parameters_frame_{int(sample)}.txt"), delimiter=',', header=0, dtype=np.float32)

            if mult_idx_tuples == []:
                column_names = row.columns
                column_categories = ['frame'] + ['parameters'] * self.num_params + ['key_colors'] * 9
                mult_idx_tuples = list(zip(column_categories, column_names)) + [('gram_paths', p_col) for p_col in self.gram_path_cols]

            for i in range(5):
                row[self.gram_path_cols[i]] = os.path.join(self.gram_directory, f"{sample}_{i}")

            rows.append(row)

        self.dataset = pd.concat(rows)
        self.dataset['frame'] = self.dataset['frame'].astype(int)
        multi_index = pd.MultiIndex.from_tuples(mult_idx_tuples)
        self.dataset.columns = multi_index

        if save_df_to is not None:
            self.dataset.to_csv(save_df_to, index=False)

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, index):

        dataset_row = self.dataset.iloc[index]
        input_grams = [torch.load(gram_filename) for gram_filename in dataset_row['gram_paths']]
        input_gram_diags = [gram[np.triu_indices(gram.shape[0], k=1)] for gram in input_grams]
        input_grams_concat = torch.cat(input_gram_diags)

        params = dataset_row['parameters'].values.astype(np.float32)

        if self.add_key_colors_to_input:
            params = np.concatenate((params, dataset_row['key_colors'].values))

        if self.feature_mask is not None:
            input_grams_concat = input_grams_concat[self.feature_mask]

        params = torch.tensor(params)

        return (input_grams_concat, params)

    def set_feature_mask(self, mask):
        self.feature_mask = mask


def check_accuracy(loader, model, verbose=False):
    with torch.no_grad():
        diff = 0
        count = 0
        for data, labels in loader:
            # print(data)
            data = data.to(device=device)
            labels = labels.to(device=device)
            # data = data.reshape(data.shape[0], -1)

            scores = model(data.float())

            diff += torch.mean(torch.absolute(scores - labels))
            if verbose:
                # pdb.set_trace()
                print("\033[95m" + str(torch.mean(data)) + " - " + str(torch.median(data)))
                print("\033[93m" + str(scores.float()))
                print("\x1B[37m" + str(labels.float()))
            # print(diff)
            count += 1

    return 1 - (diff / count)



class NN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NN, self).__init__()
        self.fc1 = nn.Linear(input_size, 2000)
        self.fc2 = nn.Linear(2000, 1500)
        self.fc3 = nn.Linear(1500, 1000)
        self.fc4 = nn.Linear(1000, 1000)
        self.fc5 = nn.Linear(1000, 1000)
        self.fc6 = nn.Linear(1000, 500)
        self.fc7 = nn.Linear(500, 100)
        self.fc8 = nn.Linear(100, 100)
        self.fc9 = nn.Linear(100, num_classes)
        self.apply(self.init_weights)


    def init_weights(self, m):
        if isinstance(m, nn.Linear):
            # He initialization for ReLU activation
            init.kaiming_uniform_(m.weight, nonlinearity='relu')
            if m.bias is not None:
                init.constant_(m.bias, 0)

    def forward(self, x):
        """
        x here is the mnist images and we run it through fc1, fc2 that we created above.
        we also add a ReLU activation function in between and for that (since it has no parameters)
        I recommend using nn.functional (F)
        """
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)
        x = self.fc4(x)
        x = F.relu(x)
        x = self.fc5(x)
        x = F.relu(x)
        x = self.fc6(x)
        x = F.relu(x)
        x = self.fc7(x)
        x = F.relu(x)
        x = self.fc8(x)
        x = F.relu(x)
        x = self.fc9(x)
        x = torch.sigmoid(x)
        return x


# @title Train

import time


def train():

    save_path = os.path.join(root_dir, "Models", wandb_run.id)
    os.makedirs(save_path, exist_ok=True)

    step = 0

    best_loss = float('inf')
    for epoch in range(config.num_epochs):

        for batch_idx, (data, targets) in (enumerate(train_loader)):
            # Get data to cuda if possible
            data = data.to(device=device)
            targets = targets.to(device=device)

            # Get to correct shape
            # data = data.reshape(data.shape[0], -1)

            # forward
            predictions = model(data.float())
            loss = criterion(predictions, targets.float())

            wandb.log({"loss": loss, "lr": scheduler.get_last_lr()[0], "epoch":epoch}, step=step)

            # backward
            optimizer.zero_grad()
            loss.backward()

            # gradient descent or adam step
            optimizer.step()


        if loss.item() < best_loss:
            best_loss = loss.item()
            try:
                torch.save(model, os.path.join(save_path, "best.model"))
            except Exception as e:
                print(f"Saving model didn't work: {e}")

        if (epoch % 100 == 0):
            print(str(time.ctime()) + "  -  epoch: " + str(epoch))
            acc = check_accuracy(test_loader, model)

            trainAcc = check_accuracy(train_loader, model)

            wandb.log({"Train Acc": trainAcc, "Test Acc": acc}, step=step)
            print(f"Accuracy on test/train set: {acc :.2f}/{trainAcc :.2f}")

            # torch.cuda.empty_cache()

        if (epoch % 400 == 0):
            try:
                torch.save(model, os.path.join(save_path, f"{str(epoch)}.model"))
            except Exception as e:
                print(f"saving model didnt work: {e}")

        scheduler.step()
        step += 1

    print(f"Accuracy on training set: {check_accuracy(train_loader, model):.2f}")
    print(f"Accuracy on test set: {check_accuracy(test_loader, model):.2f}")



def get_feature_importance(dataset):
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.multioutput import MultiOutputRegressor

    # Assuming `X` is your input matrix with shape [n_samples, 300000] and `y` are your targets

    # from cuml.ensemble import RandomForestRegressor as cuRF
    save_dir = os.path.join(root_dir, "feature_importance")
    os.makedirs(save_dir, exist_ok=True)

    feature_importances = []

    num_checked = 0
    num_same = 0

    prev_means = None

    for data, targets in tqdm(DataLoader(dataset, batch_size=100, shuffle=True, num_workers=2), total=(len(dataset) // 100) + 1):

        model = MultiOutputRegressor(RandomForestRegressor(n_estimators=50, n_jobs=-1))
        model.fit(data.numpy(), targets.numpy())
        feature_importance = model.estimators_[0].feature_importances_
        feature_importances.append(feature_importance)
        num_checked += len(data)

        mean = np.mean(feature_importances, axis=0)
        np.save(os.path.join(save_dir, f"feature_importance_{num_checked}.npy"), mean)

        if prev_means is not None:
            diff = np.abs(prev_means - mean)
            print(np.mean(diff))
            if np.all(diff < 0.001):
                num_same += 1
            else:
                num_same = 0

        prev_means = mean

        if num_same > 3:
            break

        print(f"importances did barely change for the last {num_same} batches")

    return mean


class MSLELoss(nn.Module):
    def __init__(self, logPart=15):
        super().__init__()
        self.mse = nn.MSELoss()
        self.a = logPart

    def forward(self, pred, actual):
        ret = self.mse(pred, actual) + self.a * (self.mse(torch.log(pred + 0.0000001), torch.log(actual + 0.0000001)))
        return ret


def get_file_with_highest_number(folder_path, name_pattern):
    """
    Returns the path to the file in the specified folder that matches the name pattern
    with the highest running number.

    Args:
        folder_path (str): The path to the folder containing the files.
        name_pattern (str): The name pattern to match files.

    Returns:
        str: The path to the file with the highest running number.
    """
    pattern = re.compile(name_pattern.replace("##", r"(\d+)"))

    highest_number = -1
    highest_file = None

    # List all files in the folder
    for file_name in os.listdir(folder_path):
        match = pattern.match(file_name)
        if match:
            # Extract the running number
            number = int(match.group(1))
            if number > highest_number:
                highest_number = number
                highest_file = file_name

    if highest_file:
        return os.path.join(folder_path, highest_file)
    else:
        return None

if __name__ == '__main__':

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("running on " + str(device))

    wandb_run = wandb.init(project='newDataset2024', entity='liquidmasl', save_code=True)

    # input_size = 304416  # Number of input pixels, this is just the grams and (optionally) 3 key colors
    input_size = 2413
    num_classes = 41  # number of parameters
    init_lr = 0.0001
    lr_step_size = 1000
    lr_gamma = 1
    batch_size = 32
    num_epochs = 5000
    loss_log_part = 20

    # Load Training and Test data
    # train_dataset = datasets.MNIST(root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
    # test_dataset = datasets.MNIST(root="dataset/", train=False, transform=transforms.ToTensor(), download=True)

    root_dir = R'/app/data'

    dataset = BlenderSuperShaderRendersDataset(param_dir=r'/app/data/Frames',
                                               gram_directory=r'/app/data/grams/normalised',
                                               prefetched_path=r'/app/data/prefetched_dataset.csv',
                                               safe_or_load_prefetch='load')

    # feature_importance = get_feature_importance(dataset)
    # np.save(os.path.join(root_dir, "feature_importance.npy"), feature_importance)


    latest_feature_importance_filepath = get_file_with_highest_number(os.path.join(root_dir, "feature_importance"), "feature_importance_##.npy")
    feature_importance = np.load(latest_feature_importance_filepath)

    highest_or_non_zero_mask = feature_importance > 0.0001

    non_zero_indices = np.where(highest_or_non_zero_mask > 0)[0]
    non_zero_importances = highest_or_non_zero_mask[non_zero_indices]

    # Sort the non-zero importances and get indices in descending order of importance
    sorted_indices = np.argsort(non_zero_importances)[::-1]

    # Select the top n importances or all if there are less than n
    top_indices = sorted_indices[:min(input_size, len(sorted_indices))]

    # Get the actual indices of the top features
    top_feature_indices = non_zero_indices[top_indices]

    dataset.set_feature_mask(top_feature_indices)
    input_size = len(top_feature_indices)

    print(input_size)

    # randomly split training and testing set
    # train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0, 2)), torch.utils.data.Subset(dataset, [10])
    # train_dataset, test_dataset = torch.utils.data.random_split(dataset, [9900, 99])
    train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0, 100)), torch.utils.data.Subset(dataset, range(101, 111))


    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4,
                               pin_memory=True, persistent_workers=True)
    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, num_workers=4,
                               pin_memory=True, persistent_workers=True)

    # Initialize network
    model = NN(input_size=input_size, num_classes=num_classes).to(device)
    wandb.watch(model)

    criterion = MSLELoss(loss_log_part)
    optimizer = optim.Adam(model.parameters(), lr=init_lr)
    scheduler = StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)

    print(f"Accuracy on training set: {check_accuracy(train_loader, model):.2f}")
    print(f"Accuracy on test set: {check_accuracy(test_loader, model):.2f}")

    config = wandb.config

    config.input_size = input_size  # Number of input pixels, this is just the grams and 3 key colors
    config.num_classes = num_classes  # number of parameters
    config.init_lr = init_lr
    config.lr_step_size = lr_step_size
    config.lr_gamma = lr_gamma
    config.batch_size = batch_size
    config.num_epochs = num_epochs
    config.loss_log_part = loss_log_part

    config.criterion = str(criterion)
    config.NN = str(model)

    train()


def test_train(data_loader):
    start = time.time()
    for _ in tqdm(range(10)):
        for x in data_loader:
            pass
    end = time.time()
    return end - start


if __name__ == 'dfg__main__':
    batch_size = 64

    root_dir = R'/data/'
    dataset = BlenderSuperShaderRendersDataset(param_dir=r'/app/data/Frames', gram_directory=r'/app/data/grams/normalised', prefetched_path=r'/app/data/prefetched_dataset.csv', safe_or_load_prefetch='load')
    train_num = 100
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_num, len(dataset) - 100])

    train_loader1 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,
                               pin_memory=True)
    train_loader2 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4,
                               pin_memory=True, persistent_workers=True)
    train_loader3 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,
                               pin_memory=True, persistent_workers=True)

    print(test_train(train_loader1))
    print(test_train(train_loader2))
    print(test_train(train_loader3))
