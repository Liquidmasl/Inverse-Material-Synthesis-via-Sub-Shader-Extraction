{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRgSIjZNhIXz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA_-vLFMpiI-"
      },
      "source": [
        "#@title Datastore Class\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Singleton(type):\n",
        "    _instances = {}\n",
        "    def __call__(cls, *args, **kwargs):\n",
        "        if cls not in cls._instances:\n",
        "            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\n",
        "        return cls._instances[cls]\n",
        "\n",
        "class Datastore(metaclass=Singleton):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.data = {}\n",
        "\n",
        "  def get_size(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def get_data(self, annotations, root_dir):\n",
        "\n",
        "    size = len(annotations.index)\n",
        "\n",
        "    if len(self.data) != size:\n",
        "      print(\"Preloading dataset\")\n",
        "      for i in tqdm(range(0,size)):\n",
        "\n",
        "        learnData = torch.Tensor()\n",
        "        for j in range(5):\n",
        "\n",
        "          data_path = os.path.join(root_dir,'GramsVer2', annotations.iloc[i,0] +str(j))\n",
        "          learnData = torch.cat((learnData, torch.load(data_path).flatten()),0)\n",
        "          #learnData.append(torch.load(data_path).flatten())\n",
        "\n",
        "        data_path = os.path.join(root_dir,'CoreColors', annotations.iloc[i,0] + \"_Colors.npy\")\n",
        "\n",
        "        #learnData.append(np.load(data_path).flatten())\n",
        "        learnData = torch.cat((learnData, torch.from_numpy(np.load(data_path).flatten())),0)\n",
        "        self.data[i] = learnData\n",
        "        #print(learnData)\n",
        "\n",
        "\n",
        "      #print(\"data already preloaded, returning\")\n",
        "    return self.data\n",
        "\n",
        "  def reset():\n",
        "    self.data = {}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e31R4hY3NPRM",
        "cellView": "code"
      },
      "source": [
        "#@title Dataset Class\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "#from skimage import io\n",
        "\n",
        "class smallShaderDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = pd.read_csv(os.path.join(root_dir,csv_file))\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.data_store = Datastore()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # if self.data_store is None:\n",
        "    #   data_path = os.path.join(self.root_dir,'LearnDataCombined', self.annotations.iloc[index,0] + \".npy\")\n",
        "    #   input = np.load(data_path)\n",
        "    # else:\n",
        "    input = self.data_store.get_data(annotations=self.annotations, root_dir=self.root_dir)[index]\n",
        "\n",
        "    parameters = self.annotations.iloc[index,1:]\n",
        "    parameters = np.array([parameters],dtype = float).flatten()\n",
        "    input = np.array(input,dtype = float)\n",
        "\n",
        "    sample = {'input': input, 'parameters': parameters}\n",
        "\n",
        "    #Maybe think about transforms for data augmentation. Not sure if\n",
        "    #augmentation is possible on gram matrices. maybe they have to be done before the gram calculation is done\n",
        "    #if self.transform:\n",
        "    #  sample = self.transform(sample)\n",
        "\n",
        "    #sample.ToTensor()\n",
        "\n",
        "    #print(input, parameters)\n",
        "\n",
        "    return input, parameters\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUBMyDnH2Tsd"
      },
      "source": [
        "#Define, Train and Test the NN\n",
        "\n",
        "This code is from a youtuber I found who does a great job explaining what he is doing. it is origianally made to classify hand written digits.\n",
        "I will try to get it to work for my case here. Problem might be that parametrization and classifications is a very different problem.\n",
        "On the other side if I understand NN correctly, it shouldnt really make a difference.\n",
        "https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_simple_fullynet.py\n",
        "\n",
        "very good resource aswell: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "Dataloader seams to have a problmem with my Dataset. Error message is very much not helpful. Maybe I have to write my custom \"collate\" function. I am not yet sure what that function is doing though.\n",
        "Maybe I have an error in my __getitem__ function in the Dataset class. But as far as i see are both parts of the tuple lists. which should be fine. Atleast I thought so.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVZhixWh_dbx",
        "cellView": "code"
      },
      "source": [
        "#@title Accuracy funcion\n",
        "#@markdown Check accuracy on training & test to see how good our model\n",
        "#@markdown this does not work for this task, needs a complete rework, linear difference between parameters might be enough (hopefully), if not, render image, make grams, compare grams?\n",
        "import pdb\n",
        "\n",
        "def check_accuracy(loader, model, verbose=False):\n",
        "\n",
        "  with torch.no_grad():\n",
        "        diff = 0\n",
        "        count = 0\n",
        "        for data, labels in loader:\n",
        "            #print(data)\n",
        "            data = data.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            #data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "            scores = model(data.float())\n",
        "\n",
        "            diff += torch.mean(torch.absolute(scores - labels))\n",
        "            if verbose:\n",
        "              #pdb.set_trace()\n",
        "              print(\"\\033[95m\" + str(torch.mean(data)) + \" - \" + str(torch.median(data)))\n",
        "              print(\"\\033[93m\" + str(scores.float()))\n",
        "              print(\"\\x1B[37m\" + str(labels.float()))\n",
        "            #print(diff)\n",
        "            count += 1\n",
        "\n",
        "\n",
        "\n",
        "  return 1 - (diff / count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkOY8ywM2Whx"
      },
      "source": [
        "\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torchvision # torch package for vision related things\n",
        "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
        "#import torchvision.datasets as datasets  # Standard datasets\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "from torch import nn  # All neural network modules\n",
        "from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "output_layer_names = ['block1_conv2',\n",
        "                      'block2_conv2',\n",
        "                      'block3_conv4',\n",
        "                      'block4_conv4']\n",
        "\n",
        "# Here we create our simple neural network. For more details here we are subclassing and\n",
        "# inheriting from nn.Module, this is the most general way to create your networks and\n",
        "# allows for more flexibility. I encourage you to also check out nn.Sequential which\n",
        "# would be easier to use in this scenario but I wanted to show you something that\n",
        "# \"always\" works.\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NN, self).__init__()\n",
        "        # Our first linear layer take input_size, in this case 348170 node (!!!) compressing that to 300 sucks. how to deal with that much data?\n",
        "        self.fc1 = nn.Linear(input_size, 1000, bias=False)\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "        self.fc3 = nn.Linear(1000, 100)\n",
        "        # self.fc4 = nn.Linear(100, 100)\n",
        "        # self.fc5 = nn.Linear(100, 100)\n",
        "        # self.fc6 = nn.Linear(100, 100)\n",
        "        # self.fc7 = nn.Linear(100, 100)\n",
        "        # self.fc8 = nn.Linear(100, 100)\n",
        "        self.fc9 = nn.Linear(100, num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x here is the mnist images and we run it through fc1, fc2 that we created above.\n",
        "        we also add a ReLU activation function in between and for that (since it has no parameters)\n",
        "        I recommend using nn.functional (F)\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.fc4(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc5(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc6(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc7(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc8(x)\n",
        "        # x = F.relu(x)\n",
        "        x = self.fc9(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train\n",
        "\n",
        "import time\n",
        "import pdb\n",
        "\n",
        "def train():\n",
        "\n",
        "  # Train Network\n",
        "  torch.save(model, root_dir + \"\\Models\\\\epoch_init.model\")\n",
        "  for epoch in range(config.num_epochs):\n",
        "\n",
        "\n",
        "      for batch_idx, (data, targets) in (enumerate(train_loader)):\n",
        "          # Get data to cuda if possible\n",
        "          data = data.to(device=device)\n",
        "          targets = targets.to(device=device)\n",
        "\n",
        "          # Get to correct shape\n",
        "          #data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "          # forward\n",
        "          scores = model(data.float())\n",
        "          loss = criterion(scores, targets.float())\n",
        "          #pdb.set_trace()\n",
        "\n",
        "          #pdb.set_trace()\n",
        "          wandb.log({\"loss\": loss})\n",
        "\n",
        "          # backward\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "\n",
        "          #print(\"targets: \" + str(targets))\n",
        "          #print(\"scores: \" + str(scores))\n",
        "\n",
        "          #pdb.set_trace()\n",
        "          # gradient descent or adam step\n",
        "          optimizer.step()\n",
        "          #print(\"done step\")\n",
        "\n",
        "\n",
        "\n",
        "      if (epoch % 20 == 0):\n",
        "        print(str(time.ctime()) + \"  -  epoch: \"+str(epoch))\n",
        "        acc = check_accuracy(test_loader, model)\n",
        "\n",
        "        trainAcc = check_accuracy(train_loader, model)\n",
        "\n",
        "        wandb.log({ \"Train Acc\": trainAcc, \"Test Acc\": acc })\n",
        "        print(f\"Accuracy on test/train set: {acc :.2f}/{trainAcc :.2f}\")\n",
        "\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "      if (epoch % 400 == 0):\n",
        "        acc = check_accuracy(train_loader, model, verbose=True)\n",
        "        try:\n",
        "          torch.save(model, root_dir + \"\\Models\\\\epoch_\"+str(epoch)+\".model\")\n",
        "        except Exception as e:\n",
        "          print(\"saving model didnt work\")\n",
        "\n",
        "\n",
        "  print(f\"Accuracy on training set: {check_accuracy(train_loader, model):.2f}\")\n",
        "  print(f\"Accuracy on test set: {check_accuracy(test_loader, model):.2f}\")"
      ],
      "metadata": {
        "id": "Yv7_99PYjWun",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "hQukqpp0JNrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mean Squared Log-scaled Error Loss\n",
        "import pdb\n",
        "\n",
        "class MSLELoss(nn.Module):\n",
        "    def __init__(self, logPart = 15):\n",
        "      super().__init__()\n",
        "      self.mse = nn.MSELoss()\n",
        "      self.a = logPart\n",
        "\n",
        "    def forward(self, pred, actual):\n",
        "      ret = self.mse(pred,actual) + self.a * (self.mse(torch.log(pred+0.0000001),torch.log(actual+0.0000001)))\n",
        "      return ret"
      ],
      "metadata": {
        "cellView": "code",
        "id": "LpBFh5SoIYuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "code",
        "id": "HfTaF8dJ_zHE",
        "outputId": "74b7a470-fd8c-4dd3-cfcd-2fe2867e4f88"
      },
      "source": [
        "#@title Here is the learning part\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"running on \" + str(device))\n",
        "\n",
        "def in_colab():\n",
        "  import sys\n",
        "  return 'google.colab' in sys.modules\n",
        "\n",
        "\n",
        "\n",
        "wandb.init(project='blender small stone dataset', entity='liquidmasl')\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "# Hyperparameters of our neural network which depends on the dataset, and\n",
        "# also just experimenting to see what works well (learning rate for example).\n",
        "config.input_size = 610313   #Number of input pixels, this is just the grams and 3 key colors\n",
        "config.num_classes = 9       #number of parameters\n",
        "config.learning_rate = 0.0000001\n",
        "config.batch_size = 1\n",
        "config.num_epochs = 5000\n",
        "config.criterion = \"\"\n",
        "config.NN = \"\"\n",
        "config.loss_log_part = 20\n",
        "\n",
        "# Load Training and Test data\n",
        "#train_dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "#test_dataset = datasets.MNIST(root=\"dataset/\", train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if in_colab():\n",
        "  #root_dir = '/content/drive/MyDrive/Datasets/MiniStoneShaderDataset'\n",
        "  root_dir = '/content/drive/MyDrive/Datasets/MiniStoneShaderDataset'\n",
        "else:\n",
        "  #root_dir = 'D:\\Dropbox\\Masl Stuff\\BachelorArbeit\\Google drive (to colab)\\Datasets\\MiniStoneShaderDataset'\n",
        "\n",
        "  root_dir = R'C:\\Users\\mwinkelmueller\\Dropbox\\Masl Stuff\\BachelorArbeit\\Datasets\\BiggerShaderDataset'\n",
        "\n",
        "\n",
        "if __name__ ==  '__main__':\n",
        "  dataset = smallShaderDataset(csv_file='Labels.csv', root_dir=root_dir )\n",
        "  print(len(dataset))\n",
        "\n",
        "\n",
        "# randomly split training and testing set\n",
        "  train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0,2)),torch.utils.data.Subset(dataset, [10])\n",
        "  #train_dataset, test_dataset = torch.utils.data.random_split(dataset, [9900, 99])\n",
        "  #train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0,100)),torch.utils.data.Subset(dataset, range(101,111))\n",
        "\n",
        "  train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory= True) #, persistent_workers = True)\n",
        "  test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0, pin_memory= True) #, persistent_workers = True)\n",
        "\n",
        "  # Initialize network\n",
        "  model = NN(input_size=config.input_size, num_classes=config.num_classes).to(device)\n",
        "  config.NN = str(model)\n",
        "  wandb.watch(model)\n",
        "\n",
        "  # Loss and optimizer\n",
        "  #criterion = nn.CrossEntropyLoss() # cross entropy loss is used for probability outputs, so this might not be optimal at all.\n",
        "  #criterion = nn.MSELoss()\n",
        "  #criterion = nn.L1Loss()\n",
        "  criterion = MSLELoss(config.loss_log_part)\n",
        "  config.criterion = str(criterion)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "  print(f\"Accuracy on training set: {check_accuracy(train_loader, model):.2f}\")\n",
        "  print(f\"Accuracy on test set: {check_accuracy(test_loader, model):.2f}\")\n",
        "\n",
        "  train()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliquidmasl\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/liquidmasl/blender%20small%20stone%20dataset/runs/3e90n718\" target=\"_blank\">effortless-eon-249</a></strong> to <a href=\"https://wandb.ai/liquidmasl/blender%20small%20stone%20dataset\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9999\n",
            "Preloading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9999/9999 [06:30<00:00, 25.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.75\n",
            "Accuracy on test set: 0.81\n",
            "Thu Jan 13 22:54:57 2022  -  epoch: 0\n",
            "Accuracy on test/train set: 0.81/0.75\n",
            "\u001b[95mtensor(0.3897, device='cuda:0', dtype=torch.float64) - tensor(0.3744, device='cuda:0', dtype=torch.float64)\n",
            "\u001b[93mtensor([[0.4924, 0.4913, 0.4976, 0.4884, 0.4988, 0.5098, 0.5073, 0.4943, 0.4972]],\n",
            "       device='cuda:0')\n",
            "\u001b[37mtensor([[0.2463, 0.2197, 0.4590, 0.0975, 0.1096, 0.7192, 0.8364, 0.5400, 0.9497]],\n",
            "       device='cuda:0')\n",
            "\u001b[95mtensor(0.3900, device='cuda:0', dtype=torch.float64) - tensor(0.3748, device='cuda:0', dtype=torch.float64)\n",
            "\u001b[93mtensor([[0.4924, 0.4912, 0.4977, 0.4883, 0.4987, 0.5098, 0.5075, 0.4944, 0.4972]],\n",
            "       device='cuda:0')\n",
            "\u001b[37mtensor([[0.0410, 0.0398, 0.4492, 0.5020, 0.4727, 0.2448, 0.8921, 0.1143, 0.3635]],\n",
            "       device='cuda:0')\n",
            "Thu Jan 13 22:55:08 2022  -  epoch: 20\n",
            "Accuracy on test/train set: 0.82/0.77\n",
            "Thu Jan 13 22:55:11 2022  -  epoch: 40\n",
            "Accuracy on test/train set: 0.83/0.79\n",
            "Thu Jan 13 22:55:15 2022  -  epoch: 60\n",
            "Accuracy on test/train set: 0.84/0.80\n",
            "Thu Jan 13 22:55:18 2022  -  epoch: 80\n",
            "Accuracy on test/train set: 0.83/0.81\n",
            "Thu Jan 13 22:55:21 2022  -  epoch: 100\n",
            "Accuracy on test/train set: 0.83/0.82\n",
            "Thu Jan 13 22:55:24 2022  -  epoch: 120\n",
            "Accuracy on test/train set: 0.83/0.82\n",
            "Thu Jan 13 22:55:28 2022  -  epoch: 140\n",
            "Accuracy on test/train set: 0.83/0.82\n",
            "Thu Jan 13 22:55:31 2022  -  epoch: 160\n",
            "Accuracy on test/train set: 0.83/0.82\n",
            "Thu Jan 13 22:55:34 2022  -  epoch: 180\n",
            "Accuracy on test/train set: 0.83/0.82\n",
            "Thu Jan 13 22:55:38 2022  -  epoch: 200\n",
            "Accuracy on test/train set: 0.82/0.82\n",
            "Thu Jan 13 22:55:41 2022  -  epoch: 220\n",
            "Accuracy on test/train set: 0.82/0.82\n",
            "Thu Jan 13 22:55:44 2022  -  epoch: 240\n",
            "Accuracy on test/train set: 0.82/0.82\n",
            "Thu Jan 13 22:55:48 2022  -  epoch: 260\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:55:51 2022  -  epoch: 280\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:55:54 2022  -  epoch: 300\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:55:57 2022  -  epoch: 320\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:01 2022  -  epoch: 340\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:04 2022  -  epoch: 360\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:07 2022  -  epoch: 380\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:11 2022  -  epoch: 400\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "\u001b[95mtensor(0.3897, device='cuda:0', dtype=torch.float64) - tensor(0.3744, device='cuda:0', dtype=torch.float64)\n",
            "\u001b[93mtensor([[0.1090, 0.0987, 0.5058, 0.2138, 0.2139, 0.5394, 0.7050, 0.2671, 0.5637]],\n",
            "       device='cuda:0')\n",
            "\u001b[37mtensor([[0.2463, 0.2197, 0.4590, 0.0975, 0.1096, 0.7192, 0.8364, 0.5400, 0.9497]],\n",
            "       device='cuda:0')\n",
            "\u001b[95mtensor(0.3900, device='cuda:0', dtype=torch.float64) - tensor(0.3748, device='cuda:0', dtype=torch.float64)\n",
            "\u001b[93mtensor([[0.1088, 0.0984, 0.5056, 0.2138, 0.2138, 0.5395, 0.7051, 0.2668, 0.5635]],\n",
            "       device='cuda:0')\n",
            "\u001b[37mtensor([[0.0410, 0.0398, 0.4492, 0.5020, 0.4727, 0.2448, 0.8921, 0.1143, 0.3635]],\n",
            "       device='cuda:0')\n",
            "Thu Jan 13 22:56:19 2022  -  epoch: 420\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:22 2022  -  epoch: 440\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:26 2022  -  epoch: 460\n",
            "Accuracy on test/train set: 0.82/0.83\n",
            "Thu Jan 13 22:56:29 2022  -  epoch: 480\n",
            "Accuracy on test/train set: 0.82/0.83\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\MWINKE~1\\AppData\\Local\\Temp/ipykernel_17916/3072229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy on test set: {check_accuracy(test_loader, model):.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\MWINKE~1\\AppData\\Local\\Temp/ipykernel_17916/118219908.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m           \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m           \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m           \u001b[1;31m#print(\"targets: \" + str(targets))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[1;32m~\\anaconda3\\envs\\cuda\\lib\\site-packages\\wandb\\wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\cuda\\lib\\site-packages\\wandb\\wandb_torch.py\u001b[0m in \u001b[0;36m_callback\u001b[1;34m(grad, log_track)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlog_track_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\cuda\\lib\\site-packages\\wandb\\wandb_torch.py\u001b[0m in \u001b[0;36mlog_tensor_stats\u001b[1;34m(self, tensor, name)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# Remove nans from tensor. There's no good way to represent that in histograms.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mflat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mflat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mflat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;31m# Often the whole tensor is nan or inf. Just don't log it in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.28 GiB (GPU 0; 24.00 GiB total capacity; 18.78 GiB already allocated; 1.38 GiB free; 20.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%debug\n"
      ],
      "metadata": {
        "id": "kx_nNdynzWBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40385b6b-0198-4a29-fe55-6c35fd21248c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[1;32mc:\\users\\mwinke~1\\appdata\\local\\temp\\ipykernel_1972\\3724494547.py\u001b[0m(38)\u001b[0;36mget_data\u001b[1;34m()\u001b[0m\n",
            "\n",
            "ipdb> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i,j = \"\",\"\"\n",
        "for i  in train_loader:\n",
        "  print(\"i = \" + str(i))\n",
        "  print(\"j = \" + str(j))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i79DFwHH6En",
        "outputId": "65178a5d-1608-4f12-dee1-9ce2751c976f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i = tensor([[0.2463, 0.2197, 0.4590, 0.0975, 0.1096, 0.7192, 0.8364, 0.5400, 0.9497]],\n",
            "       dtype=torch.float64)\n",
            "j = \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "-wH9tUw2MI58",
        "outputId": "2340d2ea-9c91-47d0-a7bb-f65155a4b0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\MWINKE~1\\AppData\\Local\\Temp/ipykernel_12964/2516562487.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install steps on new machine:\n",
        "\n",
        "1. install conda\n",
        "2. make new environment\n",
        "3. enter environment\n",
        "4. https://pytorch.org/get-started/locally/#anaconda (use most up to date cuda that is advertised there, not newest from nvidea\n",
        "5. install jupyter\n",
        "6. install jupyter over https https://research.google.com/colaboratory/local-runtimes.html\n",
        "6. install dependencies:"
      ],
      "metadata": {
        "id": "Oa1k-tyeTQ9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install wandb -qqq"
      ],
      "metadata": {
        "id": "mQrbswDbUQXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def train(data_loader):\n",
        "    start = time.time()\n",
        "    for _ in tqdm(range(10)):\n",
        "        for x in data_loader:\n",
        "            pass\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset = torchvision.datasets.FashionMNIST(\n",
        "        root=\".\", train=True, download=True,\n",
        "        transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "    batch_size = 32\n",
        "    train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
        "    train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
        "    train_loader3 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8, persistent_workers=True)\n",
        "    train_loader4 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=10, persistent_workers=True)\n",
        "\n",
        "    print(train(train_loader1))\n",
        "    print(train(train_loader2))\n",
        "    print(train(train_loader3))\n",
        "    print(train(train_loader4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7K_79qp0xm",
        "outputId": "9970a644-e0ba-4a6f-dfc3-1bca8ffc701a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.0144419670105\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:26<00:00,  2.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26.934943437576294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.53588342666626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.765934228897095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.\n",
        "from tqdm import tqdm  # For nice progress bar!\n",
        "import numpy as np\n",
        "\n",
        "def train(data_loader):\n",
        "    start = time.time()\n",
        "    for _ in tqdm(range(10)):\n",
        "        for x in data_loader:\n",
        "            pass\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "if __name__ ==  '__main__':\n",
        "  batch_size = 64\n",
        "\n",
        "  root_dir = R'C:\\Users\\mwinkelmueller\\Desktop\\MiniStoneShaderDataset\\MiniStoneShaderDataset'\n",
        "  dataset = smallShaderDataset(csv_file='Labels.csv', root_dir=root_dir )\n",
        "  train_dataset, test_dataset = torch.utils.data.random_split(dataset, [99, 9900])\n",
        "\n",
        "  train_loader1 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory= True)\n",
        "  train_loader2 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory= True, persistent_workers = True)\n",
        "  train_loader3 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory= True, persistent_workers = True)\n",
        "\n",
        "  print(train(train_loader1))\n",
        "  print(train(train_loader2))\n",
        "  print(train(train_loader3))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qv_0BrDnG6V",
        "outputId": "22e458d2-0399-4205-9da5-88de3773f8f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.980217933654785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|                                                                                                                                                                                               | 0/10 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KbEFdVfzS8F",
        "outputId": "c9573b79-aeaa-4664-da73-222a0bc79ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[1;32mc:\\users\\mwinkelmueller\\anaconda3\\envs\\cuda\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m(83)\u001b[0;36m__getattr__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m     81 \u001b[1;33m            \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     82 \u001b[1;33m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m---> 83 \u001b[1;33m            \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     84 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[1;32m     85 \u001b[1;33m    \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> \n",
            "--KeyboardInterrupt--\n",
            "\n",
            "KeyboardInterrupt: Interrupted by user\n",
            "ipdb> q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dd0zw94-zUfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ozyEuYQdyA5X"
      },
      "source": [
        "#@title Dataset Class without datastore singletop cause threading\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "#from skimage import io\n",
        "\n",
        "class smallShaderDataset(Dataset):\n",
        "  def __init__(self, csv_file, root_dir, transform=None):\n",
        "    self.annotations = pd.read_csv(os.path.join(root_dir,csv_file))\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    #self.data_store = Datastore()\n",
        "    #self.data = {}\n",
        "\n",
        "    #print(\"Preloading dataset\")\n",
        "    #for i in tqdm(range(0,len(self.annotations.index))):\n",
        "    #  data_path = os.path.join(root_dir,'LearnDataCombined', self.annotations.iloc[i,0] + \".npy\")\n",
        "    #  self.data[i] = np.load(data_path).astype(float)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    # if self.data_store is None:\n",
        "    #   data_path = os.path.join(self.root_dir,'LearnDataCombined', self.annotations.iloc[index,0] + \".npy\")\n",
        "    #   input = np.load(data_path)\n",
        "    # else:\n",
        "    #   input = self.data_store.get_data(annotations=self.annotations,root_dir=self.root_dir)[index]\n",
        "\n",
        "    #input = self.data[index]\n",
        "    data_path = os.path.join(self.root_dir,'LearnDataCombined', self.annotations.iloc[index,0] + \".npy\")\n",
        "    input = np.load(data_path)\n",
        "\n",
        "\n",
        "    parameters = self.annotations.iloc[index,1:]\n",
        "    parameters = np.array([parameters],dtype = float).flatten()\n",
        "    input = np.array(input,dtype = float)\n",
        "\n",
        "    sample = {'input': input, 'parameters': parameters}\n",
        "\n",
        "    #Maybe think about transforms for data augmentation. Not sure if\n",
        "    #augmentation is possible on gram matrices. maybe they have to be done before the gram calculation is done\n",
        "    #if self.transform:\n",
        "    #  sample = self.transform(sample)\n",
        "\n",
        "    #sample.ToTensor()\n",
        "\n",
        "    return input, parameters\n",
        "    #return sample\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn  # All neural network modules\n",
        "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "x = torch.randn(10, 10)\n",
        "y = torch.randn(10, 10).double()\n",
        "\n",
        "loss = criterion(x, y)\n",
        "print(loss)\n",
        "\n",
        "class MSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, actual):\n",
        "        return self.mse(pred,actual)\n",
        "\n",
        "my_criterion = MSLELoss()\n",
        "loss = my_criterion(x, y)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "id": "rMp1-wMuoTTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e99c55-281b-4565-dab8-cb24b198ed67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.9248, dtype=torch.float64)\n",
            "tensor(1.9248, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 3)\n",
        "y = torch.randn(5,7)\n"
      ],
      "metadata": {
        "id": "Tu4bA0xCHgZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat((x.flatten(), y.flatten()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg95za3eHnBH",
        "outputId": "3f6f66a8-28d1-49b4-cecf-6795b1035025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.7520, -0.1125,  0.4215,  1.0126, -0.8815, -0.1612, -0.1587, -0.2043,\n",
              "        -1.2234, -0.0809, -1.4768,  0.8540, -1.0764, -1.5306, -0.1756, -1.2939,\n",
              "        -0.5519,  0.0408, -1.0841,  0.0418,  0.9888, -0.4683,  2.1372,  0.9524,\n",
              "         0.3356,  0.0187,  1.0995, -0.5083,  0.2861,  0.5331, -0.5142, -0.5931,\n",
              "         1.0444, -1.0340,  1.2568, -1.5478,  0.5090, -0.8003,  1.7369, -0.2373,\n",
              "         0.7872])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ju_4Xs_7HrEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}