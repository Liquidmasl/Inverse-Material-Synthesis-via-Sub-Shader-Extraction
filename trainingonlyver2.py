# -*- coding: utf-8 -*-
"""TrainingOnlyVer2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cmgqsNTS8qiqrKoq-65VROnD3kOleTv7
"""

from torch.utils.data import Dataset
import torch
import torch.nn.functional as F  # Parameterless functions, like (some) activation functions
from torch import optim  # For optimizers like SGD, Adam, etc.
from torch import nn  # All neural network modules
from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.
from tqdm import tqdm  # For nice progress bar!
import wandb
import torch
import torchvision
import time
from torch.utils.data import DataLoader  # Gives easier dataset managment by creating mini batches etc.
from tqdm import tqdm  # For nice progress bar!
import numpy as np
import pandas as pd
import os
# @title Datastore Class


class Singleton(type):
    _instances = {}

    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class Datastore(metaclass=Singleton):

    def __init__(self):
        self.data = {}

    def get_size(self):
        return len(self.data)

    def get_data(self, annotations, root_dir):

        size = len(annotations.index)

        if len(self.data) != size:
            print("Preloading dataset")
            for i in tqdm(range(0, size)):

                learnData = torch.Tensor()
                for j in range(5):
                    data_path = os.path.join(root_dir, 'GramsVer2', annotations.iloc[i, 0] + str(j))
                    learnData = torch.cat((learnData, torch.load(data_path).flatten()), 0)
                    # learnData.append(torch.load(data_path).flatten())

                data_path = os.path.join(root_dir, 'CoreColors', annotations.iloc[i, 0] + "_Colors.npy")

                # learnData.append(np.load(data_path).flatten())
                learnData = torch.cat((learnData, torch.from_numpy(np.load(data_path).flatten())), 0)
                self.data[i] = learnData
                # print(learnData)

            # print("data already preloaded, returning")
        return self.data

    def reset(self):
        self.data = {}


# @title Dataset Class


class blenderSuperShaderRendersDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.annotations = pd.read_csv(os.path.join(root_dir, csv_file))
        self.root_dir = root_dir
        self.transform = transform
        self.data_store = Datastore()

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, index):
        # if self.data_store is None:
        #   data_path = os.path.join(self.root_dir,'LearnDataCombined', self.annotations.iloc[index,0] + ".npy")
        #   input = np.load(data_path)
        # else:
        input = self.data_store.get_data(annotations=self.annotations, root_dir=self.root_dir)[index]

        parameters = self.annotations.iloc[index, 1:]
        parameters = np.array([parameters], dtype=float).flatten()
        input = np.array(input, dtype=float)

        # sample = {'input': input, 'parameters': parameters}

        # Maybe think about transforms for data augmentation. Not sure if
        # augmentation is possible on gram matrices. maybe they have to be done before the gram calculation is done
        # if self.transform:
        #  sample = self.transform(sample)

        # sample.ToTensor()

        # print(input, parameters)

        return input, parameters


"""#Define, Train and Test the NN

This code is from a youtuber I found who does a great job explaining what he is doing. it is origianally made to classify hand written digits.
I will try to get it to work for my case here. Problem might be that parametrization and classifications is a very different problem.
On the other side if I understand NN correctly, it shouldnt really make a difference.
https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_simple_fullynet.py

very good resource aswell: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html

Dataloader seams to have a problmem with my Dataset. Error message is very much not helpful. Maybe I have to write my custom "collate" function. I am not yet sure what that function is doing though.
Maybe I have an error in my __getitem__ function in the Dataset class. But as far as i see are both parts of the tuple lists. which should be fine. Atleast I thought so.

"""

# @title Accuracy funcion
# @markdown Check accuracy on training & test to see how good our model
# @markdown this does not work for this task, needs a complete rework, linear difference between parameters might be enough (hopefully), if not, render image, make grams, compare grams?
import pdb


def check_accuracy(loader, model, verbose=False):
    with torch.no_grad():
        diff = 0
        count = 0
        for data, labels in loader:
            # print(data)
            data = data.to(device=device)
            labels = labels.to(device=device)
            # data = data.reshape(data.shape[0], -1)

            scores = model(data.float())

            diff += torch.mean(torch.absolute(scores - labels))
            if verbose:
                # pdb.set_trace()
                print("\033[95m" + str(torch.mean(data)) + " - " + str(torch.median(data)))
                print("\033[93m" + str(scores.float()))
                print("\x1B[37m" + str(labels.float()))
            # print(diff)
            count += 1

    return 1 - (diff / count)


# Imports


# Here we create our simple neural network. For more details here we are subclassing and
# inheriting from nn.Module, this is the most general way to create your networks and
# allows for more flexibility. I encourage you to also check out nn.Sequential which
# would be easier to use in this scenario but I wanted to show you something that
# "always" works.
class NN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NN, self).__init__()
        # Our first linear layer take input_size, in this case 348170 node (!!!) compressing that to 300 sucks. how to deal with that much data?
        self.fc1 = nn.Linear(input_size, 1000, bias=False)
        self.fc2 = nn.Linear(1000, 1000)
        self.fc3 = nn.Linear(1000, 100)
        # self.fc4 = nn.Linear(100, 100)
        # self.fc5 = nn.Linear(100, 100)
        # self.fc6 = nn.Linear(100, 100)
        # self.fc7 = nn.Linear(100, 100)
        # self.fc8 = nn.Linear(100, 100)
        self.fc9 = nn.Linear(100, num_classes, bias=False)

    def forward(self, x):
        """
        x here is the mnist images and we run it through fc1, fc2 that we created above.
        we also add a ReLU activation function in between and for that (since it has no parameters)
        I recommend using nn.functional (F)
        """
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)
        # x = self.fc4(x)
        # x = F.relu(x)
        # x = self.fc5(x)
        # x = F.relu(x)
        # x = self.fc6(x)
        # x = F.relu(x)
        # x = self.fc7(x)
        # x = F.relu(x)
        # x = self.fc8(x)
        # x = F.relu(x)
        x = self.fc9(x)
        x = torch.sigmoid(x)
        return x


# @title Train

import time
import pdb


def train():
    # Train Network
    # torch.save(model, root_dir + "\Models\\epoch_init.model")
    best_loss = float('inf')
    for epoch in range(config.num_epochs):

        for batch_idx, (data, targets) in (enumerate(train_loader)):
            # Get data to cuda if possible
            data = data.to(device=device)
            targets = targets.to(device=device)

            # Get to correct shape
            # data = data.reshape(data.shape[0], -1)

            # forward
            predictions = model(data.float())
            loss = criterion(predictions, targets.float())

            wandb.log({"loss": loss})

            # backward
            optimizer.zero_grad()
            loss.backward()

            # gradient descent or adam step
            optimizer.step()

        if loss.item() < best_loss:
            best_loss = loss.item()
            try:
                torch.save(model, os.path.join(root_dir, wandb_run.id, "Models", "best.model"))
            except Exception as e:
                print(f"Saving model didn't work: {e}")

        if (epoch % 20 == 0):
            print(str(time.ctime()) + "  -  epoch: " + str(epoch))
            acc = check_accuracy(test_loader, model)

            trainAcc = check_accuracy(train_loader, model)

            wandb.log({"Train Acc": trainAcc, "Test Acc": acc})
            print(f"Accuracy on test/train set: {acc :.2f}/{trainAcc :.2f}")

            torch.cuda.empty_cache()

        if (epoch % 400 == 0):
            try:
                torch.save(model, os.path.join(root_dir, wandb_run.id, "Models", f"{str(epoch)}.model"))
            except Exception as e:
                print(f"saving model didnt work: {e}")

    print(f"Accuracy on training set: {check_accuracy(train_loader, model):.2f}")
    print(f"Accuracy on test set: {check_accuracy(test_loader, model):.2f}")


"""https://stats.stackexchange.com/questions/261704/training-a-neural-network-for-regression-always-predicts-the-mean

![image.png](data:image/png;base64,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)
"""

# @title Mean Squared Log-scaled Error Loss
import pdb


class MSLELoss(nn.Module):
    def __init__(self, logPart=15):
        super().__init__()
        self.mse = nn.MSELoss()
        self.a = logPart

    def forward(self, pred, actual):
        ret = self.mse(pred, actual) + self.a * (self.mse(torch.log(pred + 0.0000001), torch.log(actual + 0.0000001)))
        return ret


# @title Here is the learning part
import matplotlib.pyplot as plt

# Set device cuda for GPU if it's available otherwise run on the CPU
if __name__ == 'n__main__':


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("running on " + str(device))

    wandb_run = wandb.init(project='bigger dataset', entity='liquidmasl')

    config = wandb.config

    # Hyperparameters of our neural network which depends on the dataset, and
    # also just experimenting to see what works well (learning rate for example).
    config.input_size = 610313  # Number of input pixels, this is just the grams and 3 key colors
    config.num_classes = 9  # number of parameters
    config.learning_rate = 0.0000001
    config.batch_size = 1
    config.num_epochs = 5000
    config.criterion = ""
    config.NN = ""
    config.loss_log_part = 20

    # Load Training and Test data
    # train_dataset = datasets.MNIST(root="dataset/", train=True, transform=transforms.ToTensor(), download=True)
    # test_dataset = datasets.MNIST(root="dataset/", train=False, transform=transforms.ToTensor(), download=True)

    root_dir = R'/data'


    dataset = blenderSuperShaderRendersDataset(csv_file='Labels.csv', root_dir=root_dir)
    print(len(dataset))

    # randomly split training and testing set
    # train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0, 2)), torch.utils.data.Subset(dataset, [10])
    # train_dataset, test_dataset = torch.utils.data.random_split(dataset, [9900, 99])
    train_dataset, test_dataset = torch.utils.data.Subset(dataset, range(0,100)),torch.utils.data.Subset(dataset, range(101,111))

    num_workers = 4
    train_loader = DataLoader(dataset=train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers,
                              pin_memory=True)  # , persistent_workers = True)
    test_loader = DataLoader(dataset=test_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers,
                             pin_memory=True)  # , persistent_workers = True)

    # Initialize network
    model = NN(input_size=config.input_size, num_classes=config.num_classes).to(device)
    config.NN = str(model)
    wandb.watch(model)

    # Loss and optimizer
    # criterion = nn.CrossEntropyLoss() # cross entropy loss is used for probability outputs, so this might not be optimal at all.
    # criterion = nn.MSELoss()
    # criterion = nn.L1Loss()
    criterion = MSLELoss(config.loss_log_part)
    config.criterion = str(criterion)
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)

    print(f"Accuracy on training set: {check_accuracy(train_loader, model):.2f}")
    print(f"Accuracy on test set: {check_accuracy(test_loader, model):.2f}")

    train()

# Commented out IPython magic to ensure Python compatibility.
# %debug
#
#
# import time
# from tqdm import tqdm
#
#
# def train(data_loader):
#     start = time.time()
#     for _ in tqdm(range(10)):
#         for x in data_loader:
#             pass
#     end = time.time()
#     return end - start
#
#
# if __name__ == '__main__':
#     train_dataset = torchvision.datasets.FashionMNIST(
#         root=".", train=True, download=True,
#         transform=torchvision.transforms.ToTensor()
#     )
#
#     batch_size = 32
#     train_loader1 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,
#                                                 pin_memory=True, num_workers=0)
#     train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,
#                                                 pin_memory=True, num_workers=8)
#     train_loader3 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,
#                                                 pin_memory=True, num_workers=8, persistent_workers=True)
#     train_loader4 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,
#                                                 pin_memory=True, num_workers=10, persistent_workers=True)
#
#     print(train(train_loader1))
#     print(train(train_loader2))
#     print(train(train_loader3))
#     print(train(train_loader4))
#
#
#

def train(data_loader):
    start = time.time()
    for _ in tqdm(range(10)):
        for x in data_loader:
            pass
    end = time.time()
    return end - start


if __name__ == '__main__':
    batch_size = 64

    root_dir = R'/data/'
    dataset = blenderSuperShaderRendersDataset(csv_file='Labels.csv', root_dir=root_dir)
    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [99, 9900])

    train_loader1 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,
                               pin_memory=True)
    train_loader2 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4,
                               pin_memory=True, persistent_workers=True)
    train_loader3 = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,
                               pin_memory=True, persistent_workers=True)

    print(train(train_loader1))
    print(train(train_loader2))
    print(train(train_loader3))
#
# # Commented out IPython magic to ensure Python compatibility.
# # %debug
#
#
# # @title Dataset Class without datastore singletop cause threading
#
# import os
# import pandas as pd
# import torch
# from torch.utils.data import Dataset
# import numpy as np
#
#
# # from skimage import io
#
# class smallShaderDataset(Dataset):
#     def __init__(self, csv_file, root_dir, transform=None):
#         self.annotations = pd.read_csv(os.path.join(root_dir, csv_file))
#         self.root_dir = root_dir
#         self.transform = transform
#         # self.data_store = Datastore()
#         # self.data = {}
#
#         # print("Preloading dataset")
#         # for i in tqdm(range(0,len(self.annotations.index))):
#         #  data_path = os.path.join(root_dir,'LearnDataCombined', self.annotations.iloc[i,0] + ".npy")
#         #  self.data[i] = np.load(data_path).astype(float)
#
#     def __len__(self):
#         return len(self.annotations)
#
#     def __getitem__(self, index):
#         # if self.data_store is None:
#         #   data_path = os.path.join(self.root_dir,'LearnDataCombined', self.annotations.iloc[index,0] + ".npy")
#         #   input = np.load(data_path)
#         # else:
#         #   input = self.data_store.get_data(annotations=self.annotations,root_dir=self.root_dir)[index]
#
#         # input = self.data[index]
#         data_path = os.path.join(self.root_dir, 'LearnDataCombined', self.annotations.iloc[index, 0] + ".npy")
#         input = np.load(data_path)
#
#         parameters = self.annotations.iloc[index, 1:]
#         parameters = np.array([parameters], dtype=float).flatten()
#         input = np.array(input, dtype=float)
#
#         sample = {'input': input, 'parameters': parameters}
#
#         # Maybe think about transforms for data augmentation. Not sure if
#         # augmentation is possible on gram matrices. maybe they have to be done before the gram calculation is done
#         # if self.transform:
#         #  sample = self.transform(sample)
#
#         # sample.ToTensor()
#
#         return input, parameters
#         # return sample
#
#
# import torch
# from torch import nn  # All neural network modules
# from torch import optim  # For optimizers like SGD, Adam, etc.
#
# criterion = nn.MSELoss()
#
# x = torch.randn(10, 10)
# y = torch.randn(10, 10).double()
#
# loss = criterion(x, y)
# print(loss)
#
#
# class MSLELoss(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.mse = nn.MSELoss()
#
#     def forward(self, pred, actual):
#         return self.mse(pred, actual)
#
#
# my_criterion = MSLELoss()
# loss = my_criterion(x, y)
# print(loss)
#
# x = torch.randn(2, 3)
# y = torch.randn(5, 7)
#
# torch.cat((x.flatten(), y.flatten()))
