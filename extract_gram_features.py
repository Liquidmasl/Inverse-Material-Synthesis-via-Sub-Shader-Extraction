# -*- coding: utf-8 -*-
"""Feature extraction 2.0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UfSGizMyTWj2kjyi9eWcGJWm6wV70pG
"""

import numpy as np
import OpenEXR as exr
import Imath


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import yaml

from PIL import Image
import matplotlib.pyplot as plt

import torchvision.transforms as transforms
import torchvision.models as models
import os
import copy

from tqdm import tqdm  # For nice progress bar!



def readEXR(filename, convert_to_sRGB=False):
    """Read color + depth data from EXR image file.

    Parameters
    ----------
    filename : str
        File path.

    Returns
    -------
    img : RGB or RGBA image in float32 format. Each color channel
          lies within the interval [0, 1].
          Color conversion from linear RGB to standard RGB is performed
          internally. See https://en.wikipedia.org/wiki/SRGB#The_forward_transformation_(CIE_XYZ_to_sRGB)
          for more information.

    Z : Depth buffer in float32 format or None if the EXR file has no Z channel.
    """

    exrfile = exr.InputFile(filename)
    header = exrfile.header()

    dw = header['dataWindow']
    isize = (dw.max.y - dw.min.y + 1, dw.max.x - dw.min.x + 1)

    channelData = {}

    # convert all channels in the image to numpy arrays

    for c in header['channels']:
        C = exrfile.channel(c, Imath.PixelType(Imath.PixelType.FLOAT))
        C = np.fromstring(C, dtype=np.float32)
        C = np.reshape(C, isize)

        channelData[c] = C



    colorChannels = ['R', 'G', 'B', 'A'] if 'A' in header['channels'] else ['R', 'G', 'B']
    img = np.concatenate([channelData[c][..., np.newaxis] for c in colorChannels], axis=2)

    # linear to standard RGB
    if convert_to_sRGB:
        img[..., :3] = np.where(img[..., :3] <= 0.0031308,
                                12.92 * img[..., :3],
                                1.055 * np.power(img[..., :3], 1 / 2.4) - 0.055)

    # sanitize image to be in range [0, 1]
    img = np.where(img < 0.0, 0.0, np.where(img > 1.0, 1, img))


    return img



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# desired size of the output image
imsize = 224

loader = transforms.Compose([
    # transforms.Resize([imsize, imsize]),  # scale imported image
    transforms.ToTensor()])  # transform it into a torch tensor


def image_loader(image_name):
    if (image_name.endswith(".exr")):
        image = readEXR(image_name)
    else:
        image = Image.open(image_name).convert('RGB')

    image = loader(image).unsqueeze(0)[:, :3, 16:240, 16:240]

    return image.to(device, torch.float)


def imshow(tensor, title=None):
    unloader = transforms.ToPILImage()  # reconvert into PIL image

    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it
    image = image.squeeze(0)  # remove the fake batch dimension
    image = unloader(image)
    plt.figure()
    plt.imshow(image)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


def gram_matrix(input):
    a, b, c, d = input.size()  # a=batch size(=1)


    features = input.view(a * b, c * d)  # resise F_XL into \hat F_XL

    G = torch.mm(features, features.t())  # compute the gram product

    return G.div(
        c * d)


class StyleExtract(nn.Module):

    def __init__(self):
        super(StyleExtract, self).__init__()
        self.min = float('inf')
        self.max = float('-inf')
        self.hist = np.histogram([])  # Empty histogram

    def forward(self, input):
        self.gram = gram_matrix(input)
        self.gram = self.gram.triu()

        self.min = min(self.min, torch.min(self.gram).item())
        self.max = max(self.max, torch.max(self.gram).item())
        # Update histogram
        self.hist += np.histogram(self.gram.cpu().detach().numpy(), bins=10)

        return input


class Normalization(nn.Module):
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        # .view the mean and std to make them [C x 1 x 1] so that they can
        # directly work with image Tensor of shape [B x C x H x W].
        # B is batch size. C is number of channels. H is height and W is width.
        self.mean = torch.tensor(mean).view(-1, 1, 1)
        self.std = torch.tensor(std).view(-1, 1, 1)

    def forward(self, img):
        # normalize img
        return (img - self.mean) / self.std



def get_style_model_and_style_extract_layers(cnn, normalization_mean, normalization_std,
                                             # style_img, content_img,
                                             # content_layers=content_layers_default,
                                             style_layers=['conv1_2', 'conv2_2', 'conv3_4', 'conv4_4', 'conv5_4']):
    # normalization module
    normalization = Normalization(normalization_mean, normalization_std).to(device)

    # just in order to have an iterable access to or list of content/syle
    # losses
    # content_losses = []
    style_grams = []

    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
    # to put in modules that are supposed to be activated sequentially
    model = nn.Sequential(normalization)

    i = 1  # increment every time we see a conv resets when pooling
    j = 1  # increase every time we see a pooling
    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            name = 'conv{}_{}'.format(j, i)
            i += 1
        elif isinstance(layer, nn.ReLU):
            name = 'relu{}_{}'.format(j, i)
            # The in-place version doesn't play very nicely with the ContentLoss
            # and StyleLoss we insert below. So we replace with out-of-place
            # ones here.
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool{}_{}'.format(j, i)
            layer = nn.AvgPool2d(3)  # @markdown changed to avg pool here like Match did in their paper
            j += 1
            i = 1
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn{}_{}'.format(j, i)
        else:
            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))

        model.add_module(name, layer)

        if name in style_layers:
            style_extract = StyleExtract()
            model.add_module("-----------------style_extract_{}".format(j), style_extract)
            style_grams.append(style_extract)

            if name == style_layers[-1]:
                break


    return model, style_grams  # , content_losses



def calculate_layer_grams_and_save(load_directory, save_directory, model, styles, metadata, metadataPath):


    all_files = os.listdir(load_directory)
    all_exr_files = set([file for file in all_files if file.endswith(".exr")])
    not_processed_files = all_exr_files - metadata['processed_files']

    processed = 0

    failed = []
    print(f"Generating Grams - {len(not_processed_files)} files to process")
    for filename in tqdm(not_processed_files):

        if not filename.endswith(".exr") or filename in metadata['processed_files']:
            continue

        loadPath = os.path.join(load_directory, filename)

        try:
            im = image_loader(loadPath)
        except Exception as e:
            # this is a bit scetchy, but probably it failed cause the image is faulty
            print(f"Error reading {loadPath}")
            print(f"Error reading {loadPath}: {e}")

            # move file into broken folder
            brokenPath = os.path.join(os.path.dirname(loadPath), 'broken')
            os.makedirs(brokenPath, exist_ok=True)
            os.rename(loadPath, os.path.join(brokenPath, filename))
            failed.append(filename)
            continue


        with torch.no_grad():
            model(im)

        for i, style in enumerate(styles):
            # Update overall min and max
            metadata['mins'][i] = min(metadata['mins'][i], style.min)
            metadata['maxs'][i] = max(metadata['maxs'][i], style.max)

            savePath = os.path.join(save_directory, 'not_normalised', filename.split(".")[0] + f"_{i}")

            matrix = style.gram.cpu()
            torch.save(matrix, savePath)

        metadata['processed_files'].add(filename)
        if processed % 100 == 0:
            print(f"Saving Metadata - {len(metadata['processed_files'])} / {len(all_exr_files)} processed")
            with open(metadataPath, 'w') as file:
                yaml.dump(metadata, file)
        processed += 1

    print(f"Failed to process {len(failed)} files")



if __name__ == '__main__':

    cnn = models.vgg19(pretrained=True).features.to(device).eval()

    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

    model, styles = get_style_model_and_style_extract_layers(cnn, cnn_normalization_mean, cnn_normalization_std)

    loadDirectory = r'/app/data/Frames'
    saveDirectory = r'/app/data/grams'

    # check if save directory has a metadata yaml file
    metadataPath = os.path.join(saveDirectory, 'not_normalised_metadata.yaml')
    if os.path.exists(metadataPath):
        with open(metadataPath, 'r') as file:
            metadata = yaml.load(file, Loader=yaml.FullLoader)
    else:
        metadata = {'mins': [float('inf')] * 5, 'maxs': [float('-inf')] * 5, 'processed_files': set()}

    calculate_layer_grams_and_save(loadDirectory, saveDirectory, model, styles, metadata, metadataPath)

    with open(metadataPath, 'w') as file:
        yaml.dump(metadata, file)

